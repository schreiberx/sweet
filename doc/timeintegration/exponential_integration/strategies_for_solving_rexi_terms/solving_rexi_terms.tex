%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{babel}

\makeatother

\usepackage{babel}
\begin{document}

\title{Solving REXI}


\author{Martin Schreiber <M.Schreiber@exeter.ac.uk>, ADD YOURSELF IF YOU
ADD THINGS<cool@things.hell> et al.}

\maketitle
This document serves as the basis to understand and discuss various
ways how to solve the REXI approximation given by
\[
e^{L}U_{0}\approx\sum_{i}\beta_{i}(\alpha_{i}+L)^{-1}U_{0}.
\]



\section{Problem formulation}

Exponential integrators provide a form to directly express the solution
of a linear operator (non-linear operators are not considered in this
work). For a linear PDE given by

\[
U_{t}=L(U)
\]
we can write 
\[
U(t)=e^{Lt}U(0).
\]
Furthermore, assuming that the $L$ operator is skew-Hermitian - hence
has imaginary Eigenvalues only - we can write this as a Rational approximation
of the EXponential Integrator (REXI)

\[
e^{L}U_{0}\approx\sum_{n=-N}^{N}Re\left(\beta_{n}(\alpha_{n}+L)^{-1}U_{0}\right),
\]
see \cite{Terry:High-order time-parallel approximation of evolution operators}.
This is the already simplified equation where the time step size $\tau$
is merged with $L$ which doesn't make a big difference here.


\section{Properties}

We will discuss properties and potential misunderstandings in the
REXI approximation.


\subsection{REXI Coefficient properties}

With overbar denoting the complex conjugate, the coefficients in the
REXI terms have the following properties:

\begin{equation}
\alpha_{-n}=\bar{\alpha}_{n}\label{eq:alpha_conjugate_symmetry-1}
\end{equation}


\begin{equation}
\beta_{-n}=\bar{\beta}_{n}\label{eq:beta_conjugate_symmetry}
\end{equation}


\begin{equation}
Im(\alpha_{0})=Im(\beta_{0})=0\label{eq:imaginary_a_b_zero}
\end{equation}



\subsection{Reduction of REXI terms}

Using the REXI coefficient properties we can almost half the terms
of the sum to

\begin{equation}
e^{L}U_{0}\approx\sum_{n=0}^{N}Re\left(\gamma_{n}(\alpha_{n}+L)^{-1}U_{0}\right)\label{eq:rexi_formulation}
\end{equation}
with
\[
\gamma_{n}:=\begin{cases}
\begin{array}{c}
\beta_{0}\\
2\beta_{n}
\end{array} & \begin{array}{c}
for\,n=0\\
else
\end{array}\end{cases}
\]
hence

\begin{equation}
\gamma_{-n}=\bar{\gamma}_{n}\label{eq:gamma_conjugate_symmetry}
\end{equation}



\subsection{Reutilization of REXI terms for several coarse time steps}

{[}Based on idea of Mike Ashworth{]}. Assuming that we're interested
in all the solutions at coarse time stamps $T_{n}:=n\Delta T$, is
it possible to directly compute them by reutilizing the REXI terms
of $T_{n<N}$ for $T_{N}$? Reusing REXI terms requires computing
REXI terms with the same $\alpha_{i}$ coefficients. Those alpha coefficients
are computed with

\[
\alpha_{n}:=h(\mu+i(m+k)).
\]
Here, $h$ specifies the sampling accuracy, $m$ is related to the
number of REXI terms and $k$ can be assumed constant and is related
to the number of poles for the approximation of the Gaussian function.
This shows, that REXI terms can be indeed reused! However, we shall
also take the $\beta$ terms into account to see if we can reuse the
first partial sum. These $\beta_{n}$ coefficients are given by 

\[
\beta_{n}^{Re}:=h\sum_{k=L_{1}}^{L_{2}}Re(b_{n-k})a_{k}
\]
and
\[
b_{m}=e^{-imh}e^{h^{2}}.
\]
All of these coefficients are constant for given $h$, but $L_{1/2}$
depend on all $a_{k}$ and hence the number of total REXI terms. Therefore,
we \textbf{cannot directly reuse the result of the REXI sum }of the
first coarse time step, \textbf{but the results of solving the inverse
problem} - each one depending on different $\alpha_{i}$.

Future work: Maybe a reformulation to reuse the previous sum reduction
is possible.


\subsection{Real values of exponential integrators}

Obviously, only real values should be computed by the exponential
integrator $e^{L}.$ There could be the assumption that \emph{REXI
also creates only real values with negligible imaginary values}. However,
this is not true! It holds that
\[
Im\left(\lim_{N\rightarrow\infty}\sum_{n=-N}^{N}\left(\beta_{n}(\alpha_{n}+L)^{-1}U_{0}\right)\right)\neq0.
\]
Note, that here we didn't restrict the solution to real values as
in \eqref{eq:rexi_formulation}.

\textbf{@TERRY: TODO: The way how I derived $\beta$ and $\alpha$
might be different to your way but I think it's the same. Do you agree
in the statement above?}


\section{Computing inverse of $(\alpha+L)^{-1}$}

It was suggested \cite{Terry:High-order time-parallel approximation of evolution operators}
to use a reformulation of this linear operator which is based on an
advective shallow-water formulation to compute $\eta(t+\Delta t)$
via a Helmholtz problem and then solve for both velocity components
direcctly. However, this reformulation in an ODE-oriented way was
only possible with a constant Coriolis term. Here, we will also discuss
Matrix formulations.


\subsection{Deriving Helmholtz problem for constant f SWE with matrix partitioning}

We can reformulate the SWE (see \cite{Schreiber:Understanding REXI})
into the following formulation
\[
((\alpha^{2}+f^{2})-g\bar{\eta}\Delta)\eta=\frac{f^{2}+\alpha^{2}}{\alpha}\eta_{0}-\bar{\eta}\delta_{0}-\frac{f\bar{\eta}}{\alpha}\zeta_{0}.
\]



\subsubsection{Spectral elements:}

For spectral element methods, Gunnar's method was successfully applied
to solve this.


\subsubsection{Spectral method:}

For spectral methods, we used a so-called fast Helmholtz solver to
directly solve this very efficiently.


\subsection{Deriving Helmholtz problem for f-varying SWE with matrix partitioning}

We have to find a matrix-formulation of this reformulation. Following
the derivation in \cite{Schreiber:Understanding REXI} we get the
system of equations

\[
((\alpha^{2}+f^{2})-g\bar{\eta}\Delta)\eta=\frac{f^{2}+\alpha^{2}}{\alpha}\eta_{0}-\bar{\eta}\delta_{0}-\frac{f\bar{\eta}}{\alpha}\zeta_{0}
\]
to solve for. Instead of treating every term in the linear operator
as being scalar-like, we can a partitioning of the matrix making the
linear operator a diagonal matrix L
\begin{equation}
L(U):=\left(\begin{array}{ccc}
0 & -\eta_{0}\partial_{x} & -\eta_{0}\partial_{y}\\
-g\partial_{x} & 0 & F\\
-g\partial_{y} & -F & 0
\end{array}\right)U
\end{equation}
and all other operators itself also representing a matrix formulation.
The term $F$ is then the matrix with varying Coriolis effect
\[
F:=\left[\begin{array}{ccccc}
cos(\theta_{0})\\
 & cos(\theta_{1})\\
 &  & \ldots\\
 &  &  & cos(\theta_{N-1})\\
 &  &  &  & cos(\theta_{N})
\end{array}\right]
\]
with $N$ the size of the matrix. Then we write the system to solve
for as

\[
((\alpha^{2}+F^{2})-g\bar{\eta}\Delta)\eta=\frac{F^{2}+\alpha^{2}}{\alpha}\eta_{0}-\bar{\eta}\delta_{0}-\frac{F\bar{\eta}}{\alpha}\zeta_{0}.
\]
Now the challenge is to solve for this system of equations with the
varying terms in the $F$ matrix. The $F^{2}$ terms lead to longitude-constant
$\cos^{2}(\theta)$ terms.


\subsubsection{Spectral elements method:}

For spectral element methods, Gunnar's method could be applied to
solve this.


\subsubsection{Spectral methods:}

Using spectral space, applying this term could basically mean to shift
a solution to a different spectrum. This could allow developing a
direct solver for it in spectral space. The real-to-real Fourier transformations
results in $\cos$-only eigenfunctions and could be appropriate for
this. \textbf{{[}TODO: Just a sketch. Seems to be good to be true,
hence probably wrong{]}.}


\subsection{Hybridization for SWE}

This is related to Colin's idea and is based on writing down the entire
formulation in its discretized way, hence before applying solver reformulations
as done in the previous section.

Before doing any analytical reformulations it discretizes the equations
first (e.g. on a C-grid) and then works on this reformulation.

This focuses on maintaining the conservative properties (e.g. avoiding
computational modes) first and then to solve it.

{[}TODO: Awesome formulation of hybridization on C-grid{]}


\subsection{Iterative solver with complex values}

A straight-forward approach is to use an iterative solver which supports
complex values. This means that $(\alpha+L)U=U_{0}$ is solved directly
and that's it if we could use already existing solvers.


\subsection{Reformulation to real-valued solver}

The complex-valued iterative system for $(\alpha+L)U=U_{0}$ can be
reformulated to a real valued system by treating real and imaginary
parts separately. This is based on splitting up $U=Re(U)+i\,Im(U)=U^{R}+i\,U^{I}$
(see also notes from Terry and Pedro). Similarly, we use $\alpha=Re(\alpha)+i\,Im(\alpha)=\alpha^{R}+i\alpha^{I}$.
Then the complex system of equations $(\alpha+L)\,U=U_{0}$ can be
written as
\[
\left[\begin{array}{c|c}
A^{R}+L & -A^{I}\\
\hline A^{I} & A^{R}+L
\end{array}\right]\left[\begin{array}{c}
U^{R}\\
U^{I}
\end{array}\right]=\left[\begin{array}{c}
U_{0}^{R}\\
0
\end{array}\right]
\]
with $A$ a Matrix with $\alpha$ values on the diagonal. Obviously,
the off-diagonal values in partitions given by $A^{I}$ are a pain
in the neck for iterative solvers: they are varying depending on the
number of REXI term. We again get a skew Hermitian matrix\textbf{
{[}TODO: Check the signs{]}}.


\subsection{Solving real and imaginary parts}

We can go one step further and generate a system of equations to solve
by eliminating $U^{I}$. We first solve the 2nd line for $U^{I}$:
\[
U^{I}=-(A^{R}+L)^{-1}A^{I}U^{R}.
\]
Putting this in the 1st line
\[
(A^{R}+L)U^{R}+A^{I}U^{I}=U_{0}^{R}
\]
we get
\[
(A^{R}+L)U^{R}+A^{I}(A^{R}+L)^{-1}A^{I}U^{R}=U_{0}^{R}.
\]
Solving this for $U^{R}$, we get
\[
\left((A^{R}+L)+A^{I}(A^{R}+L)^{-1}A^{I}\right)U^{R}=U_{0}^{R}.
\]
Inverting stuff is not nice and we multiply both sides from left side
with $(A^{R}+L)$ yielding the following equation:

\[
U^{R}:\,\,\left((A^{R}+L)^{2}-A^{I}A^{I}\right)U^{R}=(A^{R}+L)U_{0}^{R}
\]
We are not finished yet, since we also need the imaginary components
of $U$. The reason for this is that these components, once multiplied
with the imaginary component of $\beta$, create real values. Solving
the 1st line for $U^{R}$ gives us
\[
U^{R}=(A^{R}+L)^{-1}\left(A^{I}U^{I}+U_{0}^{R}\right)
\]
Putting this in the 2nd line, we get
\begin{eqnarray*}
A^{I}(A^{R}+L)^{-1}\left(A^{I}U^{I}+U_{0}^{R}\right)+(A^{R}+L)U^{I} & = & 0\\
A^{I}(A^{R}+L)^{-1}\left(A^{I}U^{I}\right)+(A^{R}+L)U^{I} & = & -A^{I}(A^{R}+L)^{-1}U_{0}^{R}\\
A^{I}A^{I}U^{I}+(A^{R}+L)^{2}U^{I} & = & -A^{I}U_{0}^{R}\\
\end{eqnarray*}


\[
U^{I}:\,\,\,\left(A^{I}A^{I}+(A^{R}+L)^{2}\right)U^{I}=-A^{I}U_{0}^{R}
\]


Boths things look quite ugly. However, this leads to another important
property of PinTing. This allows sovling both contributions independent
of each other. Hence, this would give us an additional degree of parallelization.


\subsection{Including $\beta$ and solving for real values only}

So far we totally ignored the $\beta$ coefficient in REXI. This forced
us to also care about the imaginary-values solution $U^{I}$. If we
would be able to put it into the inverse computation, we might be
able to compute only the real values. With $(AB)^{-1}=B^{-1}A^{-1}$
we can write 
\[
\sum_{i}(\beta_{i}^{-1})^{-1}(\alpha_{i}+L)^{-1}U_{0}=\sum_{i}(\alpha_{i}\beta_{i}^{-1}+L\beta_{i}^{-1})^{-1}U_{0}.
\]
We formulate this term to
\[
\sum_{i}(\alpha_{i}\beta_{i}^{-1}+L\beta_{i}^{-1})^{-1}U_{0}
\]
with $\beta_{i}^{-1}\alpha_{i}$ again complex valued and $L\beta_{i}^{-1}$
the linear operator scaled by $\beta_{i}^{-1}$, hence also containing
real and complex values. We can now apply the same strategy as before
by eliminating $U^{I}$ and solve for $U^{R}$. Let 
\[
M^{R}=Re(\alpha_{i}\beta_{i}^{-1}+L\beta_{i}^{-1})
\]
and

\[
M^{I}=Im(\alpha_{i}\beta_{i}^{-1}+L\beta_{i}^{-1}).
\]
This yields the SoE
\[
\left[\begin{array}{c|c}
M^{R} & -M^{I}\\
\hline M^{I} & M^{R}
\end{array}\right]\left[\begin{array}{c}
U^{R}\\
U^{I}
\end{array}\right]=\left[\begin{array}{c}
U_{0}^{R}\\
0
\end{array}\right]
\]
and further 
\[
U^{I}=\left(M^{R}\right)^{-1}\left(-M^{I}U^{R}\right).
\]
Putting this in 1st line yields
\[
M^{R}U^{R}+M^{I}\left(M^{R}\right)^{-1}M^{I}U^{R}=U_{0}^{R}
\]
\[
\left(M^{R}+M^{I}\left(M^{R}\right)^{-1}M^{I}\right)U^{R}=U_{0}^{R}
\]
We can also write

\[
\left(I+\left(\left(M^{R}\right)^{-1}M^{I}\right)^{2}\right)U^{R}=\left(M^{R}\right)^{-1}U_{0}^{R}
\]
Now the big question arises what $\left(\left(M^{R}\right)^{-1}M^{I}\right)$
is. Seems like computing the time step tendencies $L(U)$ with different
constant contributions given by the shifted poles.


\section{Interpreting $M^{R/I}$ and $(\alpha+L)^{-1}$ terms}

\textbf{{[}TODO: Here we assuming that the previous reformulations
are really possible and there are probably a lot of bugs in it{]}.}

We like to get insight in the meaning of the terms 
\[
\left(M^{R}\right)^{-1}M^{I}
\]


Both terms only consist out of a real formulation and should be summarized
here as
\[
(a+bL)
\]
with $a$ a real-valued constant which is e.g. given by $Re(\alpha_{i}\beta_{i}^{-1})$
and $bL$ the linear operator $L$ scaled by a real-valued scalar
$b$.

{[}10 minute brainstorming with John T.{]}

$(a+bL)U$ can be interpreted as an explicit time stepping method.

$(a+bL)^{-1}U$ can be interpreted as an implicit time stepping method.

The factors $a$ and $b$ can then be interpreted as scaling factors
and time step sizes.


\section{Final notes}

Have fun in reading this. Don't miss out all the errors! ;-)
\begin{thebibliography}{4}
\bibitem[4]{Schreiber:Understanding REXI}Understanding REXI

\bibitem{Schreiber:Formulations of the shallow-water equations}Formulations
of the shallow-water equations, M. Schreiber, P. Peixoto et al.

\bibitem{Terry:High-order time-parallel approximation of evolution operators}High-order
time-parallel approximation of evolution operators, T. Haut et al.

\bibitem{Moler:Nineteen Dubious Ways to Compute the Exponential of a Matrix}Nineteen
Dubious Ways to Compute the Exponential of a Matrix, Twenty-Five Years
Later, Cleve Moler and Charles Van Loan, SIAM review

\bibitem{Damle:Near optimal rational approximations of large data sets}Near
optimal rational approximations of large data sets, Damle, A., Beylkin,
G., Haut, T. S. \& Monzon\end{thebibliography}

\end{document}
