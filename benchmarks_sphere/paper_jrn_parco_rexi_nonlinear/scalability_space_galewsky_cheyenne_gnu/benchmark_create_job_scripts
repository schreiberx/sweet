#! /usr/bin/env python3

import os
import sys
import math

from itertools import product


from SWEET import *
p = SWEETJobGeneration()

verbose = False
#verbose = True

##################################################

p.compile.compiler = 'gnu'
p.compile.sweet_mpi = 'disable'
p.runtime.space_res_spectral = 128
p.runtime.reuse_plans = 2	# enforce using plans (todo, enforcing not yet implemented)!
p.compile.threading = 'omp'

p.parallelization.core_affinity = 'compact'
p.parallelization.core_oversubscription = False

gen_reference_solution = False

#p.runtime.simtime = 432000 #timestep_size_reference*(2**6)*10
p.runtime.simtime = 720 #timestep_size_reference*(2**6)*10

params_timestep_sizes_explicit = [30]
#timestep_sizes_explicit = [10, 20, 30, 60, 120, 180]

params_timestep_sizes_implicit = [30]
#timestep_sizes_implicit = [60, 120, 180, 360, 480, 600, 720]

params_timestep_sizes_rexi = [30]
#timestep_sizes_rexi = [60, 120, 180, 240, 300, 360, 480, 600, 720]

# Parallelization
#params_pspace_num_cores_per_rank = [p.platform_resources.num_cores_per_socket]
params_pspace_num_cores_per_rank = [p.platform_resources.num_cores_per_node]
#params_pspace_num_threads_per_rank = [i for i in range(1, p.platform_resources.num_cores_per_socket+1)]
params_pspace_num_threads_per_rank = [i for i in range(1, p.platform_resources.num_cores_per_node+1)]

params_ptime_num_cores_per_rank = [1]

unique_id_filter = []
unique_id_filter.append('simparams')
unique_id_filter.append('rexi_params')
unique_id_filter.append('compiler')
unique_id_filter.append('disc_space')
unique_id_filter.append('timestep_order')
unique_id_filter.append('timestep_size')
unique_id_filter.append('benchmark')

p.unique_id_filter = unique_id_filter


p.runtime.output_timestep_size = p.runtime.simtime

# No output
p.runtime.output_filename = "-"

# REXI stuff
params_ci_N = [128]
#params_ci_N = [1]
params_ci_max_imag = [30.0]
params_ci_max_real = [10.0]

##################################################


#
# Force deactivating Turbo mode
#
p.parallelization.force_turbo_off = True


def estimateWallclockTime(p):
	"""
	Return an estimated wallclock time
	"""
	#
	# Reference wallclock time and corresponding time step size
	# e.g. for explicit RK2 integration scheme
	#
	# On Cheyenne with GNU compiler
	# OMP_NUM_THREADS=18
	# 247.378 seconds for ln_erk2 with dt=30 m=128 t=432000
	#
	ref_wallclock_seconds = 60*4
	ref_simtime = 432000
	ref_timestep_size = 60
	ref_mode_res = 128

	# Use this scaling for additional wallclock time
	safety_scaling = 10
	# 5 Min additionaly
	safety_add = 60*5

	wallclock_seconds = ref_wallclock_seconds

	# inv. linear with simulation time
	wallclock_seconds *= p.runtime.simtime/ref_simtime

	# linear with time step size
	wallclock_seconds *= ref_timestep_size/p.runtime.timestep_size

	# inverse quadratic with resolution
	wallclock_seconds *= pow(p.runtime.space_res_spectral/ref_mode_res, 2.0)

	if p.runtime.rexi_method != '':
		if p.runtime.rexi_method != 'ci':
			raise Exception("TODO: Support other REXI methods")

		# Complex-valued
		wallclock_seconds *= 2.0

		# Number of REXI terms
		wallclock_seconds *= p.runtime.rexi_ci_n

		# Parallelization in time
		wallclock_seconds /= p.parallelization.pardims_dict['time'].num_ranks

	if wallclock_seconds <= 0:
		raise Exception("Estimated wallclock_seconds <= 0")

	wallclock_seconds *= safety_scaling
	wallclock_seconds += safety_add

	return wallclock_seconds

p.compile.lapack = 'enable'
p.compile.mkl = 'disable'

# Request dedicated compile script
p.compilecommand_in_jobscript = False


#
# Run simulation on plane or sphere
#
p.compile.program = 'swe_sphere'

p.compile.plane_spectral_space = 'disable'
p.compile.plane_spectral_dealiasing = 'disable'
p.compile.sphere_spectral_space = 'enable'
p.compile.sphere_spectral_dealiasing = 'enable'

p.compile.rexi_timings = 'enable'

p.compile.quadmath = 'enable'


#
# Activate Fortran source
#
p.compile.fortran_source = 'enable'


# Verbosity mode
p.runtime.verbosity = 0

#
# Mode and Physical resolution
#
p.runtime.space_res_spectral = 128
p.runtime.space_res_physical = -1

#
# Benchmark
#
p.runtime.benchmark_name = "galewsky"

#
# Compute error
#
p.runtime.compute_error = 0

#
# Preallocate the REXI matrices
#
p.runtime.rexi_sphere_preallocation = 1

#
# Deactivate instability checks
#
p.runtime.instability_checks = 0

p.compile.rexi_thread_parallel_sum = 'disable'


#
# REXI method
# N=64, SX,SY=50 and MU=0 with circle primitive provide good results
#
p.runtime.rexi_method = ''
p.runtime.rexi_ci_n = 128
p.runtime.rexi_ci_max_real = -999
p.runtime.rexi_ci_max_imag = -999
p.runtime.rexi_ci_sx = -1
p.runtime.rexi_ci_sy = -1
p.runtime.rexi_ci_mu = 0
p.runtime.rexi_ci_primitive = 'circle'

#p.runtime.rexi_beta_cutoff = 1e-16
p.runtime.rexi_beta_cutoff = 0

p.runtime.viscosity = 0.0


timestep_size_reference = params_timestep_sizes_explicit[0]


p.runtime.rexi_extended_modes = 0

# Groups to execute, see below
# l: linear
# ln: linear and nonlinear
#groups = ['l1', 'l2', 'ln1', 'ln2', 'ln4']
groups = ['ln2']





#
# allow including this file
#
if __name__ == "__main__":

	if len(sys.argv) > 1:
		groups = [sys.argv[1]]

	print("Groups: "+str(groups))

	for group in groups:
		# 1st order linear

		# 2nd order nonlinear
		if group == 'ln2':
			ts_methods = [
				['ln_erk',		4,	4,	0],	# reference solution
				['ln_erk',		2,	2,	0],

				#['lg_irk_lc_n_erk_ver0',	2,	2,	0],
				['lg_irk_lc_n_erk_ver1',	2,	2,	0],
				#['l_irk_n_erk_ver0',	2,	2,	0],
				#['l_irk_n_erk_ver1',	2,	2,	0],

				#['lg_rexi_lc_n_erk_ver0',	2,	2,	0],
				['lg_rexi_lc_n_erk_ver1',	2,	2,	0],
				#['l_rexi_n_erk_ver0',	2,	2,	0],
				#['l_rexi_n_erk_ver1',	2,	2,	0],

				['lg_rexi_lc_n_etdrk',	2,	2,	0],
				#['l_rexi_n_etdrk',	2,	2,	0],
			]

		# 4th order nonlinear
		if group == 'ln4':
			ts_methods = [
				['ln_erk',		4,	4,	0],	# reference solution
				['l_rexi_n_etdrk',	4,	4,	0],
				['ln_erk',		4,	4,	0],
			]

		#
		# Reference solution
		#
		if gen_reference_solution:
			tsm = ts_methods[0]
			p.runtime.timestep_size  = timestep_size_reference

			p.runtime.timestepping_method = tsm[0]
			p.runtime.timestepping_order = tsm[1]
			p.runtime.timestepping_order2 = tsm[2]
			p.runtime.rexi_use_direct_solution = tsm[3]

			# Update TIME parallelization
			ptime = SWEETParallelizationDimOptions('time')
			ptime.num_cores_per_rank = 1
			ptime.num_threads_per_rank = 1 #pspace.num_cores_per_rank
			ptime.num_ranks = 1

			pspace = SWEETParallelizationDimOptions('space')
			pspace.num_cores_per_rank = 1
			pspace.num_threads_per_rank = params_pspace_num_cores_per_rank[-1]
			pspace.num_ranks = 1

			# Setup parallelization
			p.setup_parallelization([pspace, ptime])

			if verbose:
				pspace.print()
				ptime.print()
				p.parallelization.print()

			if len(tsm) > 4:
				s = tsm[4]
				p.load_from_dict(tsm[4])

			p.parallelization.max_wallclock_seconds = estimateWallclockTime(p)

			p.gen_jobscript_directory('job_benchref_'+p.getUniqueID(unique_id_filter))



		#
		# Create job scripts
		#
		for tsm in ts_methods[1:]:

			p.runtime.timestepping_method = tsm[0]
			p.runtime.timestepping_order = tsm[1]
			p.runtime.timestepping_order2 = tsm[2]
			p.runtime.rexi_use_direct_solution = tsm[3]

			if len(tsm) > 4:
				s = tsm[4]
				p.runtime.load_from_dict(tsm[4])

			tsm_name = tsm[0]
			if 'ln_erk' in tsm_name:
				timestep_sizes = params_timestep_sizes_explicit
			elif 'l_erk' in tsm_name or 'lg_erk' in tsm_name:
				timestep_sizes = params_timestep_sizes_explicit
			elif 'l_irk' in tsm_name or 'lg_irk' in tsm_name:
				timestep_sizes = params_timestep_sizes_implicit
			elif 'l_rexi' in tsm_name or 'lg_rexi' in tsm_name:
				timestep_sizes = params_timestep_sizes_rexi
			else:
				print("Unable to identify time stepping method "+tsm_name)
				sys.exit(1)

			for pspace_num_cores_per_rank, pspace_num_threads_per_rank, p.runtime.timestep_size in product(params_pspace_num_cores_per_rank, params_pspace_num_threads_per_rank, timestep_sizes):
				pspace = SWEETParallelizationDimOptions('space')
				pspace.num_cores_per_rank = pspace_num_cores_per_rank
				pspace.num_threads_per_rank = pspace_num_threads_per_rank
				pspace.num_ranks = 1
				pspace.setup()


				if not '_rexi' in p.runtime.timestepping_method:
					p.runtime.rexi_method = ''

					# Update TIME parallelization
					ptime = SWEETParallelizationDimOptions('time')
					ptime.num_cores_per_rank = 1
					ptime.num_threads_per_rank = 1 #pspace.num_cores_per_rank
					ptime.num_ranks = 1
					ptime.setup()

					p.setup_parallelization([pspace, ptime])

					if verbose:
						pspace.print()
						ptime.print()
						p.parallelization.print()

					p.parallelization.max_wallclock_seconds = estimateWallclockTime(p)

					p.gen_jobscript_directory('job_bench_'+p.getUniqueID())

				else:

					for ci_N, ci_max_imag, ci_max_real in product(params_ci_N, params_ci_max_imag, params_ci_max_real):
						p.runtime.load_from_dict({
							'rexi_method': 'ci',
							'ci_n':ci_N,
							'ci_max_real':ci_max_real,
							'ci_max_imag':ci_max_imag,
							'half_poles':0,
							#'ci_gaussian_filter_scale':0.0,
							#'ci_gaussian_filter_dt_norm':0.0,	# unit scaling for T128 resolution
							#'ci_gaussian_filter_exp_N':0.0,
						})

						for time_ranks in params_ptime_num_cores_per_rank:

								# Update TIME parallelization
								ptime = SWEETParallelizationDimOptions('time')
								ptime.num_cores_per_rank = 1
								ptime.num_threads_per_rank = 1
								ptime.num_ranks = time_ranks
								ptime.setup()

								p.setup_parallelization([pspace, ptime])

								if verbose:
									pspace.print()
									ptime.print()
									p.parallelization.print()

								# Generate only scripts with max number of cores
								p.parallelization.max_wallclock_seconds = estimateWallclockTime(p)

								p.gen_jobscript_directory('job_bench_'+p.getUniqueID())

	#
	# SHTNS plan generation scripts
	#
	p.runtime.reuse_plans = 1	# search for awesome plans and store them

	#
	# Create dummy scripts to be used for SHTNS script generation
	#

	# No parallelization in time
	ptime = SWEETParallelizationDimOptions('time')
	ptime.num_cores_per_rank = 1
	ptime.num_threads_per_rank = 1
	ptime.num_ranks = 1
	ptime.setup()

	for tsm in ts_methods[1:2]:

		p.runtime.timestepping_method = tsm[0]
		p.runtime.timestepping_order = tsm[1]
		p.runtime.timestepping_order2 = tsm[2]
		p.runtime.rexi_use_direct_solution = tsm[3]

		if not '_rexi' in p.runtime.timestepping_method:
			p.runtime.rexi_method = ''
		else:
			p.runtime.rexi_method = 'ci'

		if len(tsm) > 4:
			s = tsm[4]
			p.runtime.load_from_dict(tsm[4])


		for pspace_num_cores_per_rank, pspace_num_threads_per_rank, p.runtime.timestep_size in product(params_pspace_num_cores_per_rank, params_pspace_num_threads_per_rank, timestep_sizes):
			pspace = SWEETParallelizationDimOptions('space')
			pspace.num_cores_per_rank = pspace_num_cores_per_rank
			pspace.num_threads_per_rank = pspace_num_threads_per_rank
			pspace.num_ranks = 1
			pspace.setup()

			p.setup_parallelization([pspace, ptime])

			# Use 10 minutes per default to generate plans
			p.parallelization.max_wallclock_seconds = 60*10

			# Set simtime to 0
			p.runtime.simtime = 0

			# No output
			p.runtime.output_timestep_size = -1
			p.runtime.output_filename = "-"

			p.gen_jobscript_directory('job_plan_'+p.getUniqueID())



	# Write compile script
	p.write_compilecommands("./compile_platform_"+p.platforms.platform_id+".sh")


